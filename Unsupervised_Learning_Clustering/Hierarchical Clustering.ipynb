{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Hierarchical Clustering typically comes in two flavours (essentially, bottom up or top down): \n",
    "\n",
    "* Divisive: Starts with the entire dataset comprising one cluster that is iteratively split- one point at a time- until each point forms its own cluster.\n",
    "* Agglomerative: The agglomerative method in reverse- individual points are iteratively combined until all points belong to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "np.random.seed(844)\n",
    "clust1 = np.random.normal(5, 2, (1000,2))\n",
    "clust2 = np.random.normal(15, 3, (1000,2))\n",
    "clust3 = np.random.multivariate_normal([17,3], [[1,0],[0,1]], 1000)\n",
    "clust4 = np.random.multivariate_normal([2,16], [[1,0],[0,1]], 1000)\n",
    "dataset1 = np.concatenate((clust1, clust2, clust3, clust4))\n",
    "\n",
    "# we take the first array as the second array has the cluster labels\n",
    "dataset2 = datasets.make_circles(n_samples=1000, factor=.5, noise=.05)[0]\n",
    "\n",
    "# plot clustering output on the two datasets\n",
    "def plot_two_cluster(df1, df2, color1, color2, title1,  title2):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(6*2, 3*2)\n",
    "    ax1.set_title(title1,fontsize=14)\n",
    "    ax1.set_xlim(min(df1[:,0]), max(df1[:,0]))\n",
    "    ax1.set_ylim(min(df1[:,1]), max(df1[:,1]))\n",
    "    ax1.scatter(df1[:, 0], df1[:, 1],s=8,lw=0,c= color1)\n",
    "    \n",
    "    ax2.set_title(title2,fontsize=14)\n",
    "    ax2.set_xlim(min(df2[:,0]), max(df2[:,0]))\n",
    "    ax2.set_ylim(min(df2[:,1]), max(df2[:,1]))\n",
    "    ax2.scatter(df2[:, 0], df2[:, 1],s=8,lw=0,c=color2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plot clustering output on the two datasets\n",
    "def plot_one_cluster(df1, color1, title1):\n",
    "    fig, (ax) = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(6*2, 3*2)\n",
    "    ax.set_title(title1,fontsize=14)\n",
    "    ax.set_xlim(min(df1[:,0]), max(df1[:,0]))\n",
    "    ax.set_ylim(min(df1[:,1]), max(df1[:,1]))\n",
    "    ax.scatter(df1[:, 0], df1[:, 1],s=8,lw=0,c= color1)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing agglomerative (bottom up) hierarchical clustering\n",
    "# we're going to specify that we want 4 and 2 clusters, respectively\n",
    "hc_dataset1 = cluster.AgglomerativeClustering(n_clusters=4, affinity='euclidean', \n",
    "                                              linkage='ward').fit_predict(dataset1)\n",
    "hc_dataset2 = cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "                                              linkage='single').fit_predict(dataset2)\n",
    "print(\"Dataset 1\")\n",
    "print(*[\"Cluster \"+str(i)+\": \"+ str(sum(hc_dataset1==i)) for i in range(4)], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_cluster(df1=dataset1, df2=dataset2, color1=hc_dataset1, color2=hc_dataset2, title1 = 'Dataset 1',  title2 = 'Dataset 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to Single linkage for clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HC with single linkage worked well but complete linkage didn't perform so well on the noisy circles. \n",
    "By imposing simple connectivity constraints (points can only cluster with their n(=5) nearest neighbours), HC captures the non-globular structures within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_dataset2 = cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "                                              linkage='complete').fit_predict(dataset2)\n",
    "connect = kneighbors_graph(dataset2, n_neighbors=5, include_self=False)\n",
    "hc_dataset2_connectivity = cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "                                              linkage='complete',connectivity=connect).fit_predict(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_cluster(dataset2, dataset2,hc_dataset2,hc_dataset2_connectivity,\n",
    "             title1='Without Connectivity', title2='With Connectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main disadvantage of HC is that it requires too much memory for large datasets (that n x n matrix blows up pretty quickly). Divisive clustering is $O(2^n)$, while agglomerative clustering comes in somewhat better at $O(n^2 log(n))$ (though special cases of $O(n^2)$ are available for single and maximum linkage agglomerative clustering)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
